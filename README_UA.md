# Опис усіх функцій
### 1. `parse_logs(file_path)`

**Опис**: Це основна функція парсингу, яка обробляє лог-файл рядок за рядком, витягує необхідну інформацію, зберігає її в структуру даних, а також рахує кількість атак (XSS, SQL ін'єкції, IDOR).

**Аргументи**:
- `file_path` (str): Шлях до файлу з логами (наприклад, `access.log`), який потрібно проаналізувати.

**Повертає**:
- `pd.DataFrame`: DataFrame, що містить IP-адреси та URL-адреси кожного запиту.
- `xss_attempts` (int): Кількість XSS атак, знайдених у логах.
- `sqli_attempts` (int): Кількість SQL ін'єкцій, знайдених у логах.
- `idor_attempts` (int): Кількість IDOR атак, знайдених у логах.

**Алгоритм**:
1. Ініціалізує порожній список `data` для зберігання інформації про IP та URL, а також лічильники атак (`xss_attempts`, `sqli_attempts`, `idor_attempts`).
2. Відкриває файл логів і читає його рядок за рядком.
3. Для кожного рядка:
   - Витягує IP-адресу за допомогою регулярного виразу.
   - Витягує URL-адресу з HTTP-запиту за допомогою регулярного виразу.
   - Перевіряє URL на наявність патернів, що вказують на XSS, SQL ін'єкції або IDOR атаки, і збільшує відповідний лічильник, якщо знаходить збіг.
   - Додає IP і URL у список `data`.
4. Повертає DataFrame, що містить IP і URL для кожного запиту, а також кількість атак для кожного типу.

**Код функції**:
```python
def parse_logs(file_path):
    data = []
    xss_attempts = 0
    sqli_attempts = 0
    idor_attempts = 0
    
    with open(file_path, 'r') as file:
        for line in file:
            ip_match = re.match(r'(\d+\.\d+\.\d+\.\d+)', line)
            url_match = re.search(r'\"[A-Z]+\s(.*?)\sHTTP', line)
            
            if ip_match and url_match:
                ip = ip_match.group(1)
                url = url_match.group(1)
                
                # Перевірка на XSS, SQL ін'єкції та IDOR
                if any(pattern in url for pattern in xss_patterns):
                    xss_attempts += 1
                if any(pattern in url for pattern in sqli_patterns):
                    sqli_attempts += 1
                if any(pattern in url for pattern in idor_patterns):
                    idor_attempts += 1
                
                data.append({'IP': ip, 'URL': url})
    
    return pd.DataFrame(data), xss_attempts, sqli_attempts, idor_attempts
```

### 2. Основний код (без іменованої функції)

Усі інші частини коду не обгорнуті в окрему функцію, але вони виконують наступні завдання:

#### Групування та сортування даних

**Опис**: Виконує групування даних за комбінацією IP-адрес і URL-адрес, а також підраховує кількість запитів для кожної комбінації. Дані потім сортуються за спаданням кількості запитів.

**Код**:
```python
ip_url_counts = data_frame.groupby(['IP', 'URL']).size().reset_index(name='Count')
ip_url_counts = ip_url_counts.sort_values(by='Count', ascending=False)
```

#### Збереження даних у CSV

**Опис**: Зберігає результати у три різні CSV-файли:
- `top_ip_url_full.csv`: Містить повний список комбінацій IP та URL із кількістю запитів для кожної комбінації.
- `top_ip_url_10.csv`: Містить топ-10 комбінацій IP та URL.
- `top_ip_url_100.csv`: Містить топ-100 комбінацій IP та URL.

**Код**:
```python
ip_url_counts.to_csv('top_ip_url_full.csv', index=False, sep='\t')
ip_url_counts.head(10).to_csv('top_ip_url_10.csv', index=False, sep='\t')
ip_url_counts.head(100).to_csv('top_ip_url_100.csv', index=False, sep='\t')
```

#### Відображення графіку

**Опис**: Будує горизонтальну стовпчасту діаграму, що показує топ-10 IP-адрес за кількістю запитів, використовуючи бібліотеку `matplotlib`.

**Код**:
```python
ip_url_counts.head(10).plot(kind='barh', x='IP', y='Count', title='Top 10 IPs by Request Count')
plt.xlabel('Number of Requests')
plt.ylabel('IP Addresses')
plt.gca().invert_yaxis()
plt.show()
```

#### Виведення статистики атак

**Опис**: Виводить у консоль кількість знайдених атак кожного типу: XSS, SQL ін'єкції та IDOR.

**Код**:
```python
print(f"Кількість XSS атак: {xss_attempts}")
print(f"Кількість SQL ін'єкцій: {sqli_attempts}")
print(f"Кількість IDOR атак: {idor_attempts}")
```

### Пропозиція: Структурувати основний код у вигляді функцій

Щоб зробити парсер більш гнучким та модульним, можна структурувати основний код у вигляді функцій:

1. **`group_and_sort_data(data_frame)`** – для групування та сортування даних.
2. **`save_to_csv(data, filename, top_n=None)`** – для збереження даних у CSV (включаючи часткові та повні звіти).
3. **`plot_top_ips(data_frame, top_n=10)`** – для побудови графіку з топ IP-адресами.

Це зробить парсер більш зручним для розширення та використання в різних сценаріях.

# Опис бібліотек

### 1. `re` – бібліотека для роботи з регулярними виразами

Бібліотека `re` (входить до стандартної бібліотеки Python) використовується для парсингу лог-файлів. Регулярні вирази дозволяють знаходити певні патерни в тексті, що є корисним для виділення частин рядків з логів, таких як IP-адреси та URL-запити.

#### У коді:
- **`re.match(r'(\d+\.\d+\.\d+\.\d+)', line)`**: використовується для знаходження IP-адреси на початку рядка.
- **`re.search(r'\"[A-Z]+\s(.*?)\sHTTP', line)`**: використовується для знаходження URL-запиту в рядку.

### 2. `pandas` – бібліотека для роботи з даними

`pandas` – це потужна бібліотека для обробки та аналізу даних, яка дозволяє працювати з таблицями даних, виконувати групування, сортування та зберігати результати в різних форматах (наприклад, у CSV).

#### У коді:
- **`pd.DataFrame(data)`**: створює таблицю (DataFrame) з даними, зібраними з лог-файлу.
- **`data_frame['IP'].value_counts().head(10)`**: підраховує кількість запитів з кожної IP-адреси, а потім вибирає топ-10 IP за кількістю запитів.
- **`ip_counts.to_csv('top_ips.csv')`**: зберігає результати в CSV-файлі, що дозволяє зберегти підраховані дані для подальшого аналізу.

### 3. `matplotlib.pyplot` – бібліотека для побудови графіків

`matplotlib.pyplot` – це бібліотека для побудови графіків у Python. Вона дозволяє створювати різноманітні графічні візуалізації, включаючи лінійні графіки, гістограми, діаграми тощо.

#### У коді:
- **`ip_counts.plot(kind='barh', title='Top 10 IPs by Request Count')`**: створює горизонтальну стовпчасту діаграму, яка показує топ-10 IP-адрес за кількістю запитів.
- **`plt.xlabel('Number of Requests')`** та **`plt.ylabel('IP Addresses')`**: встановлюють підписи для осей X та Y.
- **`plt.gca().invert_yaxis()`**: інвертує вісь Y, щоб найпопулярніші IP-адреси були зверху графіку.
- **`plt.show()`**: відображає графік на екрані.

### Коротко:

- **`re`** потрібна для парсингу тексту та виділення інформації (IP, URL).
- **`pandas`** використовується для обробки даних, групування, сортування та експорту в CSV.
- **`matplotlib.pyplot`** використовується для побудови графіка, який візуалізує результати (топ-10 IP за кількістю запитів).

Ці бібліотеки спільно дозволяють зчитати файл логів, проаналізувати його, зберегти результати та побудувати графік для легшого розуміння даних.

# Питання

#### 1. Як витягуються IP-адреси та URL із логів?

**Відповідь**:  
##### Для витягання IP-адрес та URL-адрес із логів використовуються регулярні вирази. IP-адреса витягується з початку рядка за допомогою регулярного виразу `r'(\d+\.\d+\.\d+\.\d+)'`, а URL витягується за допомогою виразу `r'\"[A-Z]+\s(.*?)\sHTTP'`, що знаходить URL-адресу між методом запиту (GET, POST тощо) і протоколом (HTTP/1.1).

---

#### 2. Як зберігаються результати аналізу?

**Відповідь**:  
##### Результати зберігаються у трьох CSV-файлах:
- **`top_ip_url_full.csv`**: містить повний список комбінацій IP-адрес і URL-адрес із кількістю запитів для кожної.
- **`top_ip_url_10.csv`**: містить топ-10 комбінацій IP-адрес і URL за кількістю запитів.
- **`top_ip_url_100.csv`**: містить топ-100 комбінацій IP і URL за кількістю запитів.  
Кожен файл використовує один символ табуляції як роздільник між колонками.

---

#### 3. Чи можна змінити кількість записів у часткових звітах?

**Відповідь**:  
##### Так, ви можете змінити кількість записів у часткових звітах, змінивши параметри `head(10)` та `head(100)` на потрібне значення, або передавши параметр до функції, якщо реорганізувати код. Наприклад, якщо потрібен топ-20, просто змініть `head(10)` на `head(20)`.

---

#### 4. Як код будує графік?

**Відповідь**:  
##### Код використовує бібліотеку `matplotlib` для побудови горизонтальної стовпчастої діаграми. Він показує топ-10 IP-адрес за кількістю запитів, де IP-адреси розташовані на осі Y, а кількість запитів – на осі X. Це дозволяє швидко побачити найбільш активні IP-адреси на сервері.

---

#### 5. Що потрібно зробити, якщо структура логів відрізняється?

**Відповідь**:  
##### Якщо структура логів відрізняється (наприклад, логи з Nginx замість Apache), необхідно змінити регулярні вирази в `parse_logs`, які використовуються для витягання IP-адрес і URL-адрес. Це дозволить адаптувати код до нового формату логів.

---

#### 6. Які бібліотеки використовуються і для чого?

**Відповідь**:  
##### Код використовує такі бібліотеки:
##### - **`re`**: для роботи з регулярними виразами, які допомагають витягувати IP-адреси та URL-адреси з логів.
##### - **`pandas`**: для обробки даних, зокрема створення DataFrame, групування, сортування та збереження у формат CSV.
##### - **`matplotlib.pyplot`**: для побудови графіка, що показує топ-10 IP-адрес за кількістю запитів.

---

#### 7. Чи є можливість розширити код для аналізу кількох файлів?

**Відповідь**:  
##### Так, код можна легко адаптувати для обробки кількох файлів, додавши цикл для читання всіх файлів у директорії логів. Кожен файл можна обробити функцією `parse_logs`, а потім об'єднати результати у загальний DataFrame перед групуванням і збереженням.

---

#### 8. Як цей код визначає часткову і повну інформацію?

**Відповідь**:  
##### Код генерує три звіти:
##### - Повний звіт (`top_ip_url_full.csv`) містить усі записи з логів, де IP-адреси поєднуються з URL-адресами.
##### - Часткові звіти (`top_ip_url_10.csv` і `top_ip_url_100.csv`) містять лише топ-10 та топ-100 записів відповідно. Це дозволяє користувачам отримати короткий звіт або детальний звіт, залежно від їхніх потреб.
